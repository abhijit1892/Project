{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd723e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 Cost: 277.78547596199525\n",
      "Iteration: 1 Cost: 268.4140907697869\n",
      "Iteration: 2 Cost: 261.18270913529346\n",
      "Iteration: 3 Cost: 254.70619195504727\n",
      "Iteration: 4 Cost: 248.69422419132306\n",
      "Iteration: 5 Cost: 243.02955811592653\n",
      "Iteration: 6 Cost: 237.64259341676922\n",
      "Iteration: 7 Cost: 232.48607079856004\n",
      "Iteration: 8 Cost: 227.52607342839798\n",
      "Iteration: 9 Cost: 222.73742812668127\n",
      "Iteration: 10 Cost: 218.10097690756317\n",
      "Iteration: 11 Cost: 213.60183789659354\n",
      "Iteration: 12 Cost: 209.22824344884876\n",
      "Iteration: 13 Cost: 204.97073624769587\n",
      "Iteration: 14 Cost: 200.82159814532562\n",
      "Iteration: 15 Cost: 196.77443610634722\n",
      "Iteration: 16 Cost: 192.82387735033328\n",
      "Iteration: 17 Cost: 188.96534212621012\n",
      "Iteration: 18 Cost: 185.19487264293147\n",
      "Iteration: 19 Cost: 181.50900317722284\n",
      "Iteration: 20 Cost: 177.90466070620548\n",
      "Iteration: 21 Cost: 174.3790883742372\n",
      "Iteration: 22 Cost: 170.92978617391972\n",
      "Iteration: 23 Cost: 167.55446469337525\n",
      "Iteration: 24 Cost: 164.2510088425843\n",
      "Iteration: 25 Cost: 161.01744924418253\n",
      "Iteration: 26 Cost: 157.85193954203407\n",
      "Iteration: 27 Cost: 154.7527383016639\n",
      "Iteration: 28 Cost: 151.71819449055627\n",
      "Iteration: 29 Cost: 148.7467357620192\n",
      "Iteration: 30 Cost: 145.8368589442801\n",
      "Iteration: 31 Cost: 142.98712227160124\n",
      "Iteration: 32 Cost: 140.19613899726795\n",
      "Iteration: 33 Cost: 137.46257210733182\n",
      "Iteration: 34 Cost: 134.78512991481526\n",
      "Iteration: 35 Cost: 132.1625623611047\n",
      "Iteration: 36 Cost: 129.59365788777873\n",
      "Iteration: 37 Cost: 127.07724077052937\n",
      "Iteration: 38 Cost: 124.61216882908565\n",
      "Iteration: 39 Cost: 122.19733144447453\n",
      "Iteration: 40 Cost: 119.83164782868741\n",
      "Iteration: 41 Cost: 117.51406550265465\n",
      "Iteration: 42 Cost: 115.24355894700957\n",
      "Iteration: 43 Cost: 113.01912839694616\n",
      "Iteration: 44 Cost: 110.83979875789403\n",
      "Iteration: 45 Cost: 108.7046186230798\n",
      "Iteration: 46 Cost: 106.61265937751236\n",
      "Iteration: 47 Cost: 104.56301437572117\n",
      "Iteration: 48 Cost: 102.55479818282512\n",
      "Iteration: 49 Cost: 100.58714587032223\n",
      "Iteration: 50 Cost: 98.65921235946519\n",
      "Iteration: 51 Cost: 96.77017180627627\n",
      "Iteration: 52 Cost: 94.91921702323316\n",
      "Iteration: 53 Cost: 93.10555893344994\n",
      "Iteration: 54 Cost: 91.32842605382854\n",
      "Iteration: 55 Cost: 89.58706400419334\n",
      "Iteration: 56 Cost: 87.88073503986175\n",
      "Iteration: 57 Cost: 86.20871760547044\n",
      "Iteration: 58 Cost: 84.57030590818074\n",
      "Iteration: 59 Cost: 82.96480950863639\n",
      "Iteration: 60 Cost: 81.39155292826561\n",
      "Iteration: 61 Cost: 79.84987527168859\n",
      "Iteration: 62 Cost: 78.33912986314742\n",
      "Iteration: 63 Cost: 76.85868389600148\n",
      "Iteration: 64 Cost: 75.40791809443506\n",
      "Iteration: 65 Cost: 73.98622638662174\n",
      "Iteration: 66 Cost: 72.59301558866369\n",
      "Iteration: 67 Cost: 71.22770509869585\n",
      "Iteration: 68 Cost: 69.88972660060001\n",
      "Iteration: 69 Cost: 68.57852377682886\n",
      "Iteration: 70 Cost: 67.2935520298795\n",
      "Iteration: 71 Cost: 66.03427821199983\n",
      "Iteration: 72 Cost: 64.80018036274089\n",
      "Iteration: 73 Cost: 63.59074745400252\n",
      "Iteration: 74 Cost: 62.40547914224215\n",
      "Iteration: 75 Cost: 61.24388552754534\n",
      "Iteration: 76 Cost: 60.10548691927309\n",
      "Iteration: 77 Cost: 58.989813608022736\n",
      "Iteration: 78 Cost: 57.89640564365778\n",
      "Iteration: 79 Cost: 56.82481261917312\n",
      "Iteration: 80 Cost: 55.77459346018056\n",
      "Iteration: 81 Cost: 54.745316219810555\n",
      "Iteration: 82 Cost: 53.73655787883727\n",
      "Iteration: 83 Cost: 52.747904150845685\n",
      "Iteration: 84 Cost: 51.778949292269296\n",
      "Iteration: 85 Cost: 50.8292959171354\n",
      "Iteration: 86 Cost: 49.8985548163633\n",
      "Iteration: 87 Cost: 48.986344781469015\n",
      "Iteration: 88 Cost: 48.09229243253712\n",
      "Iteration: 89 Cost: 47.21603205032651\n",
      "Iteration: 90 Cost: 46.357205412382676\n",
      "Iteration: 91 Cost: 45.51546163303624\n",
      "Iteration: 92 Cost: 44.69045700717209\n",
      "Iteration: 93 Cost: 43.88185485765705\n",
      "Iteration: 94 Cost: 43.089325386321605\n",
      "Iteration: 95 Cost: 42.31254552839302\n",
      "Iteration: 96 Cost: 41.55119881028228\n",
      "Iteration: 97 Cost: 40.804975210631895\n",
      "Iteration: 98 Cost: 40.073571024533685\n",
      "Iteration: 99 Cost: 39.35668873083033\n",
      "Iteration: 100 Cost: 38.65403686241786\n",
      "Iteration: 101 Cost: 37.965329879467525\n",
      "Iteration: 102 Cost: 37.29028804549124\n",
      "Iteration: 103 Cost: 36.62863730617476\n",
      "Iteration: 104 Cost: 35.980109170907646\n",
      "Iteration: 105 Cost: 35.34444059693961\n",
      "Iteration: 106 Cost: 34.72137387609686\n",
      "Iteration: 107 Cost: 34.11065652399311\n",
      "Iteration: 108 Cost: 33.51204117167267\n",
      "Iteration: 109 Cost: 32.92528545962533\n",
      "Iteration: 110 Cost: 32.35015193411354\n",
      "Iteration: 111 Cost: 31.786407945755418\n",
      "Iteration: 112 Cost: 31.233825550308485\n",
      "Iteration: 113 Cost: 30.692181411600963\n",
      "Iteration: 114 Cost: 30.161256706558124\n",
      "Iteration: 115 Cost: 29.64083703227405\n",
      "Iteration: 116 Cost: 29.13071231508007\n",
      "Iteration: 117 Cost: 28.630676721562235\n",
      "Iteration: 118 Cost: 28.140528571482044\n",
      "Iteration: 119 Cost: 27.66007025255589\n",
      "Iteration: 120 Cost: 27.18910813704963\n",
      "Iteration: 121 Cost: 26.72745250014663\n",
      "Iteration: 122 Cost: 26.274917440047762\n",
      "Iteration: 123 Cost: 25.831320799763663\n",
      "Iteration: 124 Cost: 25.396484090560886\n",
      "Iteration: 125 Cost: 24.970232417023535\n",
      "Iteration: 126 Cost: 24.552394403694638\n",
      "Iteration: 127 Cost: 24.14280212326058\n",
      "Iteration: 128 Cost: 23.741291026244333\n",
      "Iteration: 129 Cost: 23.347699872173532\n",
      "Iteration: 130 Cost: 22.96187066219065\n",
      "Iteration: 131 Cost: 22.583648573072292\n",
      "Iteration: 132 Cost: 22.212881892627536\n",
      "Iteration: 133 Cost: 21.849421956443585\n",
      "Iteration: 134 Cost: 21.493123085949826\n",
      "Iteration: 135 Cost: 21.143842527770598\n",
      "Iteration: 136 Cost: 20.801440394339334\n",
      "Iteration: 137 Cost: 20.46577960574505\n",
      "Iteration: 138 Cost: 20.136725832785718\n",
      "Iteration: 139 Cost: 19.814147441201406\n",
      "Iteration: 140 Cost: 19.497915437061543\n",
      "Iteration: 141 Cost: 19.187903413281784\n",
      "Iteration: 142 Cost: 18.88398749724577\n",
      "Iteration: 143 Cost: 18.58604629950751\n",
      "Iteration: 144 Cost: 18.293960863552343\n",
      "Iteration: 145 Cost: 18.007614616592242\n",
      "Iteration: 146 Cost: 17.72689332137438\n",
      "Iteration: 147 Cost: 17.451685028981\n",
      "Iteration: 148 Cost: 17.18188003259898\n",
      "Iteration: 149 Cost: 16.91737082223915\n",
      "Iteration: 150 Cost: 16.658052040384653\n",
      "Iteration: 151 Cost: 16.403820438548767\n",
      "Iteration: 152 Cost: 16.154574834722922\n",
      "Iteration: 153 Cost: 15.91021607169604\n",
      "Iteration: 154 Cost: 15.670646976226806\n",
      "Iteration: 155 Cost: 15.435772319050978\n",
      "Iteration: 156 Cost: 15.2054987757058\n",
      "Iteration: 157 Cost: 14.979734888154814\n",
      "Iteration: 158 Cost: 14.758391027195774\n",
      "Iteration: 159 Cost: 14.541379355635705\n",
      "Iteration: 160 Cost: 14.328613792216634\n",
      "Iteration: 161 Cost: 14.120009976276398\n",
      "Iteration: 162 Cost: 13.915485233129317\n",
      "Iteration: 163 Cost: 13.714958540151589\n",
      "Iteration: 164 Cost: 13.518350493556719\n",
      "Iteration: 165 Cost: 13.325583275846475\n",
      "Iteration: 166 Cost: 13.13658062392365\n",
      "Iteration: 167 Cost: 12.95126779785262\n",
      "Iteration: 168 Cost: 12.769571550254142\n",
      "Iteration: 169 Cost: 12.591420096321716\n",
      "Iteration: 170 Cost: 12.416743084446038\n",
      "Iteration: 171 Cost: 12.245471567435322\n",
      "Iteration: 172 Cost: 12.07753797431919\n",
      "Iteration: 173 Cost: 11.912876082723887\n",
      "Iteration: 174 Cost: 11.751420991807018\n",
      "Iteration: 175 Cost: 11.593109095740452\n",
      "Iteration: 176 Cost: 11.437878057730092\n",
      "Iteration: 177 Cost: 11.285666784561037\n",
      "Iteration: 178 Cost: 11.136415401658072\n",
      "Iteration: 179 Cost: 10.990065228650051\n",
      "Iteration: 180 Cost: 10.846558755428516\n",
      "Iteration: 181 Cost: 10.70583961868999\n",
      "Iteration: 182 Cost: 10.567852578952186\n",
      "Iteration: 183 Cost: 10.432543498034379\n",
      "Iteration: 184 Cost: 10.299859316992308\n",
      "Iteration: 185 Cost: 10.169748034498705\n",
      "Iteration: 186 Cost: 10.042158685659672\n",
      "Iteration: 187 Cost: 9.917041321258687\n",
      "Iteration: 188 Cost: 9.794346987418965\n",
      "Iteration: 189 Cost: 9.674027705675897\n",
      "Iteration: 190 Cost: 9.556036453451206\n",
      "Iteration: 191 Cost: 9.44032714492044\n",
      "Iteration: 192 Cost: 9.326854612265995\n",
      "Iteration: 193 Cost: 9.215574587307552\n",
      "Iteration: 194 Cost: 9.106443683502567\n",
      "Iteration: 195 Cost: 8.999419378309007\n",
      "Iteration: 196 Cost: 8.894459995902992\n",
      "Iteration: 197 Cost: 8.79152469024436\n",
      "Iteration: 198 Cost: 8.690573428482713\n",
      "Iteration: 199 Cost: 8.59156697469731\n",
      "Iteration: 200 Cost: 8.494466873963919\n",
      "Iteration: 201 Cost: 8.399235436741993\n",
      "Iteration: 202 Cost: 8.305835723575605\n",
      "Iteration: 203 Cost: 8.214231530101989\n",
      "Iteration: 204 Cost: 8.124387372361078\n",
      "Iteration: 205 Cost: 8.036268472400357\n",
      "Iteration: 206 Cost: 7.94984074416875\n",
      "Iteration: 207 Cost: 7.86507077969375\n",
      "Iteration: 208 Cost: 7.781925835536137\n",
      "Iteration: 209 Cost: 7.700373819516559\n",
      "Iteration: 210 Cost: 7.62038327770863\n",
      "Iteration: 211 Cost: 7.541923381692855\n",
      "Iteration: 212 Cost: 7.464963916066509\n",
      "Iteration: 213 Cost: 7.38947526620387\n",
      "Iteration: 214 Cost: 7.3154284062621615\n",
      "Iteration: 215 Cost: 7.242794887427861\n",
      "Iteration: 216 Cost: 7.171546826398824\n",
      "Iteration: 217 Cost: 7.101656894097181\n",
      "Iteration: 218 Cost: 7.0330983046086875\n",
      "Iteration: 219 Cost: 6.96584480434356\n",
      "Iteration: 220 Cost: 6.899870661414726\n",
      "Iteration: 221 Cost: 6.835150655228684\n",
      "Iteration: 222 Cost: 6.771660066285156\n",
      "Iteration: 223 Cost: 6.709374666180886\n",
      "Iteration: 224 Cost: 6.648270707813699\n",
      "Iteration: 225 Cost: 6.588324915782875\n",
      "Iteration: 226 Cost: 6.5295144769815145\n",
      "Iteration: 227 Cost: 6.471817031377475\n",
      "Iteration: 228 Cost: 6.415210662978786\n",
      "Iteration: 229 Cost: 6.359673890979846\n",
      "Iteration: 230 Cost: 6.3051856610849875\n",
      "Iteration: 231 Cost: 6.251725337005537\n",
      "Iteration: 232 Cost: 6.19927269212704\n",
      "Iteration: 233 Cost: 6.147807901343324\n",
      "Iteration: 234 Cost: 6.097311533053788\n",
      "Iteration: 235 Cost: 6.047764541320833\n",
      "Iteration: 236 Cost: 5.999148258184207\n",
      "Iteration: 237 Cost: 5.951444386128972\n",
      "Iteration: 238 Cost: 5.904634990704116\n",
      "Iteration: 239 Cost: 5.85870249328887\n",
      "Iteration: 240 Cost: 5.813629664003456\n",
      "Iteration: 241 Cost: 5.769399614761739\n",
      "Iteration: 242 Cost: 5.725995792462645\n",
      "Iteration: 243 Cost: 5.683401972317734\n",
      "Iteration: 244 Cost: 5.641602251312073\n",
      "Iteration: 245 Cost: 5.600581041795851\n",
      "Iteration: 246 Cost: 5.560323065203981\n",
      "Iteration: 247 Cost: 5.520813345901237\n",
      "Iteration: 248 Cost: 5.482037205150366\n",
      "Iteration: 249 Cost: 5.443980255200636\n",
      "Iteration: 250 Cost: 5.406628393494535\n",
      "Iteration: 251 Cost: 5.369967796990157\n",
      "Iteration: 252 Cost: 5.333984916597\n",
      "Iteration: 253 Cost: 5.298666471722827\n",
      "Iteration: 254 Cost: 5.263999444929508\n",
      "Iteration: 255 Cost: 5.229971076695496\n",
      "Iteration: 256 Cost: 5.196568860282924\n",
      "Iteration: 257 Cost: 5.163780536707173\n",
      "Iteration: 258 Cost: 5.131594089806764\n",
      "Iteration: 259 Cost: 5.099997741411851\n",
      "Iteration: 260 Cost: 5.068979946608962\n",
      "Iteration: 261 Cost: 5.038529389100369\n",
      "Iteration: 262 Cost: 5.008634976656009\n",
      "Iteration: 263 Cost: 4.9792858366561905\n",
      "Iteration: 264 Cost: 4.950471311723228\n",
      "Iteration: 265 Cost: 4.922180955440255\n",
      "Iteration: 266 Cost: 4.89440452815542\n",
      "Iteration: 267 Cost: 4.867131992869852\n",
      "Iteration: 268 Cost: 4.840353511207532\n",
      "Iteration: 269 Cost: 4.814059439465717\n",
      "Iteration: 270 Cost: 4.7882403247440095\n",
      "Iteration: 271 Cost: 4.762886901150701\n",
      "Iteration: 272 Cost: 4.737990086084737\n",
      "Iteration: 273 Cost: 4.713540976591862\n",
      "Iteration: 274 Cost: 4.689530845793393\n",
      "Iteration: 275 Cost: 4.665951139386209\n",
      "Iteration: 276 Cost: 4.642793472212558\n",
      "Iteration: 277 Cost: 4.6200496248981935\n",
      "Iteration: 278 Cost: 4.597711540557587\n",
      "Iteration: 279 Cost: 4.575771321564787\n",
      "Iteration: 280 Cost: 4.554221226388645\n",
      "Iteration: 281 Cost: 4.53305366649118\n",
      "Iteration: 282 Cost: 4.512261203287648\n",
      "Iteration: 283 Cost: 4.49183654516732\n",
      "Iteration: 284 Cost: 4.471772544573565\n",
      "Iteration: 285 Cost: 4.452062195142102\n",
      "Iteration: 286 Cost: 4.4326986288963575\n",
      "Iteration: 287 Cost: 4.413675113498601\n",
      "Iteration: 288 Cost: 4.394985049555952\n",
      "Iteration: 289 Cost: 4.376621967979993\n",
      "Iteration: 290 Cost: 4.358579527398983\n",
      "Iteration: 291 Cost: 4.3408515116216835\n",
      "Iteration: 292 Cost: 4.323431827151634\n",
      "Iteration: 293 Cost: 4.306314500750964\n",
      "Iteration: 294 Cost: 4.2894936770527625\n",
      "Iteration: 295 Cost: 4.272963616220966\n",
      "Iteration: 296 Cost: 4.256718691656831\n",
      "Iteration: 297 Cost: 4.240753387751118\n",
      "Iteration: 298 Cost: 4.225062297681057\n",
      "Iteration: 299 Cost: 4.209640121251108\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def step_gradient(X, Y, m, learning_rate):\n",
    "    m_slope = np.zeros(len(X[0]))\n",
    "    for i in range(len(X)):\n",
    "        x = X[i]\n",
    "        y = Y[i]\n",
    "        for j in range(len(x)):\n",
    "            m_slope[j] += (-2/len(X)) * (y - sum(m*x)) * x[j]\n",
    "    new_m = m - (learning_rate * m_slope)\n",
    "    return new_m\n",
    "\n",
    "def cost(m, x, y):\n",
    "    total_cost = 0\n",
    "    for i in range(len(x)):\n",
    "        total_cost += (1/len(x)) * ((y[i] - sum(m*x[i])) ** 2)\n",
    "    mse = total_cost / 2\n",
    "    return mse\n",
    "\n",
    "def gd(x, y, learning_rate, iterations):\n",
    "    m = np.zeros(len(x[0]))\n",
    "    for i in range(iterations):\n",
    "        m = step_gradient(x, y, m, learning_rate)\n",
    "        print(\"Iteration:\", i, \"Cost:\", cost(m, x, y))\n",
    "    return m\n",
    "\n",
    "def main():\n",
    "    # Load training and testing data\n",
    "    training = np.loadtxt('D:/Train.csv', delimiter=',')\n",
    "    testing = np.loadtxt('D:/Test.csv', delimiter=',')\n",
    "    \n",
    "    # Separate features and target\n",
    "    x = training[:, :-1]\n",
    "    y = training[:, -1]\n",
    "    \n",
    "    # Feature engineering (if justified)\n",
    "    # Example: adding squared values and interactions\n",
    "    sq_features = []\n",
    "    for i in range(len(x[0])):\n",
    "        for j in range(i, len(x[0])):\n",
    "            for k in range(j, len(x[0])):\n",
    "                sq_features.append(x[:, i] * x[:, j] * x[:, k])\n",
    "    sq_features = np.array(sq_features)\n",
    "    for i in sq_features:\n",
    "        x = np.append(x, i.reshape(-1, 1), axis=1)\n",
    "    \n",
    "    # Feature scaling\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    scaler.fit(x)\n",
    "    x = scaler.transform(x)\n",
    "    \n",
    "    # Add a column of ones for bias term\n",
    "    x = np.append(x, np.ones(len(x)).reshape(-1, 1), axis=1)\n",
    "    \n",
    "    # Perform Gradient Descent\n",
    "    iterations = 300\n",
    "    learning_rate = 0.005\n",
    "    m = gd(x, y, learning_rate, iterations)\n",
    "    \n",
    "    # Prepare and scale the testing data\n",
    "    sq_features = []  # Repeat the same feature engineering on testing data\n",
    "    for i in range(len(testing[0])):\n",
    "        for j in range(i, len(testing[0])):\n",
    "            for k in range(j, len(testing[0])):\n",
    "                sq_features.append(testing[:, i] * testing[:, j] * testing[:, k])\n",
    "    sq_features = np.array(sq_features)\n",
    "    for i in sq_features:\n",
    "        testing = np.append(testing, i.reshape(-1, 1), axis=1)\n",
    "    \n",
    "    testing_scaled = scaler.transform(testing)\n",
    "    x_test = np.append(testing_scaled, np.ones(len(testing_scaled)).reshape(-1, 1), axis=1)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = np.dot(x_test, m)\n",
    "    \n",
    "    # Save predictions to a CSV file\n",
    "    np.savetxt(fname='predictions.csv', delimiter=',', fmt='%.5f', X=predictions)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
